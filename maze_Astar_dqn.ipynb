{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "maze_Astar_dqn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtsukiTaisuke/Colab/blob/master/maze_Astar_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jpj0b2UoE5H",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Solving Maze with A-star algorithm, Q-learning and Deep Q-network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BnoCO75oE5L",
        "colab_type": "text"
      },
      "source": [
        "### Objective of this notebook is to solve self-made maze with A-star algorithm, Q-learning and Deep Q-network.\n",
        "### The maze is in square shape, consists of start point, goal point and tiles in the mid of them.\n",
        "### Each tile has numericals as its point. In other words, if you step on to the tile with -1, you get 1 point subtracted.\n",
        "### The maze has blocks to prevent you from taking the route."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wc249EIoE5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pds\n",
        "import random\n",
        "import copy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from collections import deque\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nALl2hwtoE5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DMNI7gMoE5V",
        "colab_type": "text"
      },
      "source": [
        "# Maze Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWNKYfv_oE5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Maze(object):\n",
        "    def __init__(self, size=10, blocks_rate=0.1):\n",
        "        self.size = size if size > 3 else 10\n",
        "        self.blocks = int((size ** 2) * blocks_rate) \n",
        "        self.s_list = []\n",
        "        self.maze_list = []\n",
        "        self.e_list = []\n",
        "\n",
        "    def create_mid_lines(self, k):\n",
        "        if k == 0: self.maze_list.append(self.s_list)\n",
        "        elif k == self.size - 1: self.maze_list.append(self.e_list)\n",
        "        else:\n",
        "            tmp_list = []\n",
        "            for l in range(0,self.size):\n",
        "                if l == 0: tmp_list.extend(\"#\")\n",
        "                elif l == self.size-1: tmp_list.extend(\"#\")\n",
        "                else:\n",
        "                    a = random.randint(-1, 0)\n",
        "                    tmp_list.extend([a])\n",
        "            self.maze_list.append(tmp_list)\n",
        "\n",
        "    def insert_blocks(self, k, s_r, e_r):\n",
        "        b_y = random.randint(1, self.size-2)\n",
        "        b_x = random.randint(1, self.size-2)\n",
        "        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1\n",
        "        else: self.maze_list[b_y][b_x] = \"#\"\n",
        "            \n",
        "    def generate_maze(self): \n",
        "        s_r = random.randint(1, (self.size / 2) - 1)\n",
        "        for i in range(0, self.size):\n",
        "            if i == s_r: self.s_list.extend(\"S\")\n",
        "            else: self.s_list.extend(\"#\")\n",
        "        start_point = [0, s_r]\n",
        "\n",
        "        e_r = random.randint((self.size / 2) + 1, self.size - 2)\n",
        "        for j in range(0, self.size):\n",
        "            if j == e_r: self.e_list.extend([50])\n",
        "            else: self.e_list.extend(\"#\")\n",
        "        goal_point = [self.size - 1, e_r]\n",
        "\n",
        "        for k in range(0, self.size):\n",
        "            self.create_mid_lines(k)\n",
        "        \n",
        "        for k in range(self.blocks):\n",
        "            self.insert_blocks(k, s_r, e_r)\n",
        "\n",
        "        return self.maze_list, start_point, goal_point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B61n4bkBoE5d",
        "colab_type": "text"
      },
      "source": [
        "# Maze functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwzvlTvCoE5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Field(object):\n",
        "    def __init__(self, maze, start_point, goal_point):\n",
        "        self.maze = maze\n",
        "        self.start_point = start_point\n",
        "        self.goal_point = goal_point\n",
        "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
        "\n",
        "    def display(self, point=None):\n",
        "        field_data = copy.deepcopy(self.maze)\n",
        "        if not point is None:\n",
        "                y, x = point\n",
        "                field_data[y][x] = \"@@\"\n",
        "        else:\n",
        "                point = \"\"\n",
        "        for line in field_data:\n",
        "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        movables = []\n",
        "        if state == self.start_point:\n",
        "            y = state[0] + 1\n",
        "            x = state[1]\n",
        "            a = [[y, x]]\n",
        "            return a\n",
        "        else:\n",
        "            for v in self.movable_vec:\n",
        "                y = state[0] + v[0]\n",
        "                x = state[1] + v[1]\n",
        "                if not(0 < x < len(self.maze) and\n",
        "                       0 <= y <= len(self.maze) - 1 and\n",
        "                       maze[y][x] != \"#\" and\n",
        "                       maze[y][x] != \"S\"):\n",
        "                    continue\n",
        "                movables.append([y,x])\n",
        "            if len(movables) != 0:\n",
        "                return movables\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    def get_val(self, state):\n",
        "        y, x = state\n",
        "        if state == self.start_point: return 0, False\n",
        "        else:\n",
        "            v = float(self.maze[y][x])\n",
        "            if state == self.goal_point: \n",
        "                return v, True\n",
        "            else: \n",
        "                return v, False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywUOdXlRoE5i",
        "colab_type": "text"
      },
      "source": [
        "# Generate a maze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fuhX-9ooE5j",
        "colab_type": "code",
        "outputId": "dd610339-2713-46a6-9cf5-9e33b1402ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "size = 10\n",
        "barriar_rate = 0.1\n",
        "\n",
        "maze_1 = Maze(size, barriar_rate)\n",
        "maze, start_point, goal_point = maze_1.generate_maze()\n",
        "maze_field = Field(maze, start_point, goal_point)\n",
        "\n",
        "maze_field.display()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWOBBKnhoE5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf1sg4jjoE5q",
        "colab_type": "text"
      },
      "source": [
        "# Solving the maze with A-star algorithm\n",
        "### https://en.wikipedia.org/wiki/A*_search_algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_z_3sE8oE5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node(object):    \n",
        "    def __init__(self, state, start_point, goal_point):\n",
        "        self.state = state\n",
        "        self.start_point = start_point\n",
        "        self.goal_point = goal_point\n",
        "        self.hs = (self.state[0] - self.goal_point[0]) ** 2 + (self.state[1] - self.goal_point[1]) ** 2\n",
        "        self.fs = 0\n",
        "        self.parent_node = None\n",
        "    \n",
        "    def confirm_goal(self):\n",
        "        if self.goal_point == self.state: return True\n",
        "        else: return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbQZKmHMoE5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NodeList(list):\n",
        "    def find_nodelist(self, state):\n",
        "        node_list = [t for t in self if t.state==state]\n",
        "        return node_list[0] if node_list != [] else None\n",
        "    def remove_from_nodelist(self, node):\n",
        "        del self[self.index(node)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ci8L9hoE5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Aster_Solver(object):\n",
        "    def __init__(self, maze, start_point, goal_point, display=False):\n",
        "        self.Field = maze\n",
        "        self.start_point = start_point\n",
        "        self.goal_point = goal_point\n",
        "        self.open_list = NodeList()\n",
        "        self.close_list = NodeList()\n",
        "        self.steps = 0\n",
        "        self.score = 0\n",
        "        self.display = display\n",
        "        \n",
        "    def set_initial_node(self):\n",
        "        node = Node(self.start_point, self.start_point, self.goal_point)\n",
        "        node.start_point = self.start_point\n",
        "        node.goal_point = self.goal_point \n",
        "        return node\n",
        "                \n",
        "    def go_next(self, next_actions, node):\n",
        "        node_gs = node.fs - node.hs\n",
        "        for action in next_actions:\n",
        "            open_list = self.open_list.find_nodelist(action)\n",
        "            dist = (node.state[0] - action[0]) ** 2 + (node.state[1] - action[1]) ** 2\n",
        "            if open_list:\n",
        "                if open_list.fs > node_gs + open_list.hs + dist:\n",
        "                    open_list.fs = node_gs + open_list.hs + dist\n",
        "                    open_list.parent_node = node\n",
        "            else:\n",
        "                open_list = self.close_list.find_nodelist(action)\n",
        "                if open_list:\n",
        "                    if open_list.fs > node_gs + open_list.hs + dist:\n",
        "                        open_list.fs = node_gs + open_list.hs + dist\n",
        "                        open_list.parent_node = node\n",
        "                        self.open_list.append(open_list)\n",
        "                        self.close_list.remove_from_nodelist(open_list)\n",
        "                else:\n",
        "                    open_list = Node(action, self.start_point, self.goal_point)\n",
        "                    open_list.fs = node_gs + open_list.hs + dist\n",
        "                    open_list.parent_node = node\n",
        "                    self.open_list.append(open_list)\n",
        "    \n",
        "    def solve_maze(self):\n",
        "        node = self.set_initial_node()\n",
        "        node.fs = node.hs\n",
        "        self.open_list.append(node)\n",
        "        \n",
        "        while True:            \n",
        "            node = min(self.open_list, key = lambda node:node.fs)\n",
        "            print (\"current state:  {0}\".format(node.state))\n",
        "            \n",
        "            if self.display:\n",
        "                self.Field.display(node.state)\n",
        "            \n",
        "            reward, tf = self.Field.get_val(node.state)\n",
        "            self.score =  self.score + reward\n",
        "            print(\"current step: {0} \\t score: {1} \\n\".format(self.steps, self.score))\n",
        "            self.steps += 1\n",
        "            if tf == True:\n",
        "                print(\"Goal!\")\n",
        "                break\n",
        "\n",
        "            self.open_list.remove_from_nodelist(node)\n",
        "            self.close_list.append(node)\n",
        "            \n",
        "            next_actions = self.Field.get_actions(node.state)   \n",
        "            self.go_next(next_actions, node)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eny_QeSEoE52",
        "colab_type": "code",
        "outputId": "6fd799da-04fc-46e9-e121-2f6cdc3afa90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "astar_Solver = Aster_Solver(maze_field, start_point, goal_point, display=True)\n",
        "astar_Solver.solve_maze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current state:  [0, 1]\n",
            "\t  #  @@   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 0 \t score: 0 \n",
            "\n",
            "current state:  [1, 1]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  @@  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 1 \t score: -1.0 \n",
            "\n",
            "current state:  [2, 1]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  @@   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 2 \t score: -2.0 \n",
            "\n",
            "current state:  [3, 1]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  @@  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 3 \t score: -3.0 \n",
            "\n",
            "current state:  [4, 1]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #  @@   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 4 \t score: -3.0 \n",
            "\n",
            "current state:  [5, 1]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  @@   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 5 \t score: -4.0 \n",
            "\n",
            "current state:  [5, 2]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1  @@  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 6 \t score: -4.0 \n",
            "\n",
            "current state:  [5, 3]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  @@  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 7 \t score: -5.0 \n",
            "\n",
            "current state:  [6, 3]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0  @@  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 8 \t score: -5.0 \n",
            "\n",
            "current state:  [6, 4]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  @@   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 9 \t score: -6.0 \n",
            "\n",
            "current state:  [7, 4]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0  @@   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 10 \t score: -6.0 \n",
            "\n",
            "current state:  [7, 5]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0  @@   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 11 \t score: -6.0 \n",
            "\n",
            "current state:  [8, 5]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  @@   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 12 \t score: -7.0 \n",
            "\n",
            "current state:  [8, 6]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1  @@  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 13 \t score: -7.0 \n",
            "\n",
            "current state:  [8, 7]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  @@  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 14 \t score: -8.0 \n",
            "\n",
            "current state:  [9, 7]\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  @@   #   # \n",
            "current step: 15 \t score: 42.0 \n",
            "\n",
            "Goal!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0KBjICWoE55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VgOc7v8JoE58",
        "colab_type": "text"
      },
      "source": [
        "# Solving the maze in Q-learning\n",
        "### https://en.wikipedia.org/wiki/Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HQ-DgVzoE59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QLearning_Solver(object):\n",
        "    def __init__(self, maze, display=False):\n",
        "        self.Qvalue = {}\n",
        "        self.Field = maze\n",
        "        self.alpha = 0.2\n",
        "        self.gamma  = 0.9\n",
        "        self.epsilon = 0.2\n",
        "        self.steps = 0\n",
        "        self.score = 0\n",
        "        self.display = display\n",
        "\n",
        "    def qlearn(self, greedy_flg=False):\n",
        "        state = self.Field.start_point\n",
        "        while True:\n",
        "            if greedy_flg:\n",
        "                self.steps += 1\n",
        "                action = self.choose_action_greedy(state)\n",
        "                print(\"current state: {0} -> action: {1} \".format(state, action))\n",
        "                if self.display:\n",
        "                    self.Field.display(action)\n",
        "                reward, tf = self.Field.get_val(action)\n",
        "                self.score =  self.score + reward\n",
        "                print(\"current step: {0} \\t score: {1}\\n\".format(self.steps, self.score))\n",
        "                if tf == True:\n",
        "                    print(\"Goal!\")\n",
        "                    break\n",
        "            else:\n",
        "                action = self.choose_action(state)    \n",
        "            if self.update_Qvalue(state, action):\n",
        "                break\n",
        "            else:\n",
        "                state = action\n",
        "\n",
        "    def update_Qvalue(self, state, action):\n",
        "        Q_s_a = self.get_Qvalue(state, action)\n",
        "        mQ_s_a = max([self.get_Qvalue(action, n_action) for n_action in self.Field.get_actions(action)])\n",
        "        r_s_a, finish_flg = self.Field.get_val(action)\n",
        "        q_value = Q_s_a + self.alpha * ( r_s_a +  self.gamma * mQ_s_a - Q_s_a)\n",
        "        self.set_Qvalue(state, action, q_value)\n",
        "        return finish_flg\n",
        "\n",
        "\n",
        "    def get_Qvalue(self, state, action):\n",
        "        state = (state[0],state[1])\n",
        "        action = (action[0],action[1])\n",
        "        try:\n",
        "            return self.Qvalue[state][action]\n",
        "        except KeyError:\n",
        "            return 0.0\n",
        "\n",
        "    def set_Qvalue(self, state, action, q_value):\n",
        "        state = (state[0],state[1])\n",
        "        action = (action[0],action[1])\n",
        "        self.Qvalue.setdefault(state,{})\n",
        "        self.Qvalue[state][action] = q_value\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if self.epsilon < random.random():\n",
        "            return random.choice(self.Field.get_actions(state))\n",
        "        else:\n",
        "            return self.choose_action_greedy(state)\n",
        "\n",
        "    def choose_action_greedy(self, state):\n",
        "        best_actions = []\n",
        "        max_q_value = -100\n",
        "        for a in self.Field.get_actions(state):\n",
        "            q_value = self.get_Qvalue(state, a)\n",
        "            if q_value > max_q_value:\n",
        "                best_actions = [a,]\n",
        "                max_q_value = q_value\n",
        "            elif q_value == max_q_value:\n",
        "                best_actions.append(a)\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "    def dump_Qvalue(self):\n",
        "        print(\"##### Dump Qvalue #####\")\n",
        "        for i, s in enumerate(self.Qvalue.keys()):\n",
        "            for a in self.Qvalue[s].keys():\n",
        "                print(\"\\t\\tQ(s, a): Q(%s, %s): %s\" % (str(s), str(a), str(self.Qvalue[s][a])))\n",
        "            if i != len(self.Qvalue.keys())-1: \n",
        "                print('\\t------------------state   action   reward')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvuONN6oE6A",
        "colab_type": "code",
        "outputId": "7e9fa679-b852-41ab-a57d-33c4199501bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_count = 1000\n",
        "QL_solver = QLearning_Solver(maze_field, display=True)\n",
        "for i in range(learning_count):\n",
        "    QL_solver.qlearn()\n",
        "\n",
        "QL_solver.dump_Qvalue()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### Dump Qvalue #####\n",
            "\t\tQ(s, a): Q((0, 1), (1, 1)): 7.7238090485575555\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 1), (2, 1)): 9.693121165063951\n",
            "\t\tQ(s, a): Q((1, 1), (1, 2)): 9.693121165063951\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 1), (2, 2)): 11.881245738959949\n",
            "\t\tQ(s, a): Q((2, 1), (3, 1)): 11.61024573895995\n",
            "\t\tQ(s, a): Q((2, 1), (1, 1)): 7.7238090485575555\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 2), (2, 3)): 13.201384154399948\n",
            "\t\tQ(s, a): Q((2, 2), (1, 2)): 9.693121165063951\n",
            "\t\tQ(s, a): Q((2, 2), (2, 1)): 9.693121165063951\n",
            "\t\tQ(s, a): Q((2, 2), (3, 2)): 12.201384154399948\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 3), (2, 4)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((2, 3), (2, 2)): 11.881245738959949\n",
            "\t\tQ(s, a): Q((2, 3), (3, 3)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((2, 3), (1, 3)): 11.881245738959949\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 4), (3, 4)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((2, 4), (2, 5)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((2, 4), (2, 3)): 13.201384154399948\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 4), (3, 3)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((3, 4), (4, 4)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((3, 4), (3, 5)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((3, 4), (2, 4)): 14.668204615999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 3), (2, 3)): 13.201384154399948\n",
            "\t\tQ(s, a): Q((3, 3), (3, 2)): 12.201384154399948\n",
            "\t\tQ(s, a): Q((3, 3), (4, 3)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((3, 3), (3, 4)): 17.409116239999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 2), (3, 3)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((3, 2), (2, 2)): 11.881245738959949\n",
            "\t\tQ(s, a): Q((3, 2), (3, 1)): 11.61024573895995\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 3), (4, 4)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((4, 3), (3, 3)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((4, 3), (5, 3)): 19.454573599999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 4), (5, 4)): 21.727303999999943\n",
            "\t\tQ(s, a): Q((4, 4), (4, 3)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((4, 4), (4, 5)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((4, 4), (3, 4)): 17.409116239999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 4), (5, 3)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((5, 4), (4, 4)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((5, 4), (5, 5)): 25.252559999999946\n",
            "\t\tQ(s, a): Q((5, 4), (6, 4)): 24.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 3), (4, 3)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((5, 3), (5, 2)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((5, 3), (5, 4)): 21.727303999999943\n",
            "\t\tQ(s, a): Q((5, 3), (6, 3)): 22.727303999999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 2), (6, 2)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((5, 2), (5, 3)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((5, 2), (5, 1)): 15.568204615999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 2), (6, 3)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((6, 2), (5, 2)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((6, 2), (6, 1)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((6, 2), (7, 2)): 22.727303999999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 3), (5, 3)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((6, 3), (6, 2)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((6, 3), (7, 3)): 25.252559999999946\n",
            "\t\tQ(s, a): Q((6, 3), (6, 4)): 24.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 5), (3, 5)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((4, 5), (4, 4)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((4, 5), (4, 6)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((4, 5), (5, 5)): 25.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 5), (4, 5)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((3, 5), (3, 4)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((3, 5), (2, 5)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((3, 5), (3, 6)): 17.409116239999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 6), (4, 7)): 17.50911623999994\n",
            "\t\tQ(s, a): Q((4, 6), (4, 5)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((4, 6), (3, 6)): 17.409116239999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 7), (3, 7)): 15.758204615999944\n",
            "\t\tQ(s, a): Q((4, 7), (4, 6)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((4, 7), (5, 7)): 14.758204615999944\n",
            "\t\tQ(s, a): Q((4, 7), (4, 8)): 15.758204615999944\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 7), (3, 6)): 17.409116239999943\n",
            "\t\tQ(s, a): Q((3, 7), (4, 7)): 17.50911623999994\n",
            "\t\tQ(s, a): Q((3, 7), (3, 8)): 13.182384154399946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 6), (4, 6)): 19.454573599999943\n",
            "\t\tQ(s, a): Q((3, 6), (3, 5)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((3, 6), (3, 7)): 15.758204615999944\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 5), (4, 5)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((5, 5), (6, 5)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((5, 5), (5, 4)): 21.727303999999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 3), (1, 2)): 9.693121165063951\n",
            "\t\tQ(s, a): Q((1, 3), (2, 3)): 13.201384154399948\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 2), (2, 2)): 11.881245738959949\n",
            "\t\tQ(s, a): Q((1, 2), (1, 1)): 7.7238090485575555\n",
            "\t\tQ(s, a): Q((1, 2), (1, 3)): 11.881245738959949\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 1), (3, 2)): 12.201384154399948\n",
            "\t\tQ(s, a): Q((3, 1), (4, 1)): 14.011384154399948\n",
            "\t\tQ(s, a): Q((3, 1), (2, 1)): 9.693121165063951\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 7), (4, 7)): 17.50911623999994\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((3, 8), (3, 7)): 15.758204615999944\n",
            "\t\tQ(s, a): Q((3, 8), (2, 8)): 11.864145733014364\n",
            "\t\tQ(s, a): Q((3, 8), (4, 8)): 15.75820461552913\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 8), (3, 8)): 13.18238415439923\n",
            "\t\tQ(s, a): Q((4, 8), (4, 7)): 17.50911623999994\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 8), (3, 8)): 13.18238415416077\n",
            "\t\tQ(s, a): Q((2, 8), (1, 8)): 9.677729068577369\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((2, 5), (2, 4)): 14.668204615999946\n",
            "\t\tQ(s, a): Q((2, 5), (3, 5)): 20.454573599999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((4, 1), (5, 1)): 15.568204615999946\n",
            "\t\tQ(s, a): Q((4, 1), (3, 1)): 11.61024573895995\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((5, 1), (6, 1)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((5, 1), (5, 2)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((5, 1), (4, 1)): 14.011384154399948\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 1), (6, 2)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((6, 1), (5, 1)): 15.568204615999946\n",
            "\t\tQ(s, a): Q((6, 1), (7, 1)): 19.454573599999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 1), (8, 1)): 17.50911623999994\n",
            "\t\tQ(s, a): Q((7, 1), (6, 1)): 18.409116239999943\n",
            "\t\tQ(s, a): Q((7, 1), (7, 2)): 22.727303999999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 1), (7, 1)): 19.454573599999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 2), (6, 2)): 20.454573599999943\n",
            "\t\tQ(s, a): Q((7, 2), (7, 3)): 25.252559999999946\n",
            "\t\tQ(s, a): Q((7, 2), (7, 1)): 19.454573599999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 3), (7, 2)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((7, 3), (7, 4)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((7, 3), (6, 3)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((7, 3), (8, 3)): 28.058399999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 4), (7, 5)): 31.17599999999995\n",
            "\t\tQ(s, a): Q((7, 4), (7, 3)): 25.252559999999946\n",
            "\t\tQ(s, a): Q((7, 4), (8, 4)): 31.17599999999995\n",
            "\t\tQ(s, a): Q((7, 4), (6, 4)): 24.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 5), (6, 5)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((7, 5), (8, 5)): 34.63999999999995\n",
            "\t\tQ(s, a): Q((7, 5), (7, 4)): 28.058399999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 5), (7, 5)): 31.17599999999995\n",
            "\t\tQ(s, a): Q((6, 5), (5, 5)): 25.252559999999946\n",
            "\t\tQ(s, a): Q((6, 5), (6, 6)): 24.252559999999946\n",
            "\t\tQ(s, a): Q((6, 5), (6, 4)): 24.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 5), (8, 6)): 39.59999999999996\n",
            "\t\tQ(s, a): Q((8, 5), (8, 4)): 31.17599999999995\n",
            "\t\tQ(s, a): Q((8, 5), (7, 5)): 31.17599999999995\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 6), (8, 7)): 43.99999999999997\n",
            "\t\tQ(s, a): Q((8, 6), (8, 5)): 34.63999999999995\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 7), (7, 7)): 38.59999999999996\n",
            "\t\tQ(s, a): Q((8, 7), (8, 8)): 38.59999999999996\n",
            "\t\tQ(s, a): Q((8, 7), (8, 6)): 39.59999999999996\n",
            "\t\tQ(s, a): Q((8, 7), (9, 7)): 49.999999999999986\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 7), (7, 8)): 34.73999999999995\n",
            "\t\tQ(s, a): Q((7, 7), (8, 7)): 43.99999999999997\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((7, 8), (6, 8)): 31.265999999999952\n",
            "\t\tQ(s, a): Q((7, 8), (7, 7)): 38.59999999999996\n",
            "\t\tQ(s, a): Q((7, 8), (8, 8)): 38.59999999999996\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 8), (7, 8)): 34.73999999999995\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 8), (7, 8)): 34.73999999999995\n",
            "\t\tQ(s, a): Q((8, 8), (8, 7)): 43.99999999999997\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 4), (8, 3)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((8, 4), (8, 5)): 34.63999999999995\n",
            "\t\tQ(s, a): Q((8, 4), (7, 4)): 28.058399999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((8, 3), (8, 4)): 31.17599999999995\n",
            "\t\tQ(s, a): Q((8, 3), (7, 3)): 25.252559999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 4), (6, 3)): 22.727303999999943\n",
            "\t\tQ(s, a): Q((6, 4), (6, 5)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((6, 4), (7, 4)): 28.058399999999946\n",
            "\t\tQ(s, a): Q((6, 4), (5, 4)): 21.727303999999943\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((6, 6), (6, 5)): 28.058399999999946\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 8), (1, 7)): 8.698519066506417\n",
            "\t\tQ(s, a): Q((1, 8), (2, 8)): 11.864145618284068\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 7), (1, 6)): 7.597079816220573\n",
            "\t\tQ(s, a): Q((1, 7), (1, 8)): 9.676541714345634\n",
            "\t------------------state   action   reward\n",
            "\t\tQ(s, a): Q((1, 6), (1, 7)): 8.665428486806803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2ZcsIXInoE6D",
        "colab_type": "code",
        "outputId": "709b9498-f8c5-4c0b-9254-8857f47fce23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "QL_solver.qlearn(greedy_flg=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current state: [0, 1] -> action: [1, 1] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  @@  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 1 \t score: -1.0\n",
            "\n",
            "current state: [1, 1] -> action: [2, 1] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  @@   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 2 \t score: -2.0\n",
            "\n",
            "current state: [2, 1] -> action: [2, 2] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1  @@   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 3 \t score: -2.0\n",
            "\n",
            "current state: [2, 2] -> action: [2, 3] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0  @@  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 4 \t score: -2.0\n",
            "\n",
            "current state: [2, 3] -> action: [2, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  @@  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 5 \t score: -3.0\n",
            "\n",
            "current state: [2, 4] -> action: [3, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  @@   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 6 \t score: -4.0\n",
            "\n",
            "current state: [3, 4] -> action: [4, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1  @@   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 7 \t score: -4.0\n",
            "\n",
            "current state: [4, 4] -> action: [4, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0  @@  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 8 \t score: -4.0\n",
            "\n",
            "current state: [4, 5] -> action: [5, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1  @@   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 9 \t score: -4.0\n",
            "\n",
            "current state: [5, 5] -> action: [6, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1  @@  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 10 \t score: -4.0\n",
            "\n",
            "current state: [6, 5] -> action: [7, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0  @@   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 11 \t score: -4.0\n",
            "\n",
            "current state: [7, 5] -> action: [8, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  @@   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 12 \t score: -5.0\n",
            "\n",
            "current state: [8, 5] -> action: [8, 6] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1  @@  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 13 \t score: -5.0\n",
            "\n",
            "current state: [8, 6] -> action: [8, 7] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  @@  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 14 \t score: -6.0\n",
            "\n",
            "current state: [8, 7] -> action: [9, 7] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  @@   #   # \n",
            "current step: 15 \t score: 44.0\n",
            "\n",
            "Goal!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWErz8qToE6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjhFuiLaoE6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAB9QXIBoE6O",
        "colab_type": "text"
      },
      "source": [
        "# Solving the maze in Deep Q-learning\n",
        "### https://deepmind.com/research/dqn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD46WIreoE6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN_Solver:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 1.0\n",
        "        self.e_decay = 0.9999\n",
        "        self.e_min = 0.01\n",
        "        self.learning_rate = 0.0001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_shape=(2,2), activation='tanh'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='tanh'))\n",
        "        model.add(Dense(128, activation='tanh'))\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember_memory(self, state, action, reward, next_state, next_movables, done):\n",
        "        self.memory.append((state, action, reward, next_state, next_movables, done))\n",
        "\n",
        "    def choose_action(self, state, movables):\n",
        "        if self.epsilon >= random.random():\n",
        "            return random.choice(movables)\n",
        "        else:\n",
        "            return self.choose_best_action(state, movables)\n",
        "        \n",
        "    def choose_best_action(self, state, movables):\n",
        "        best_actions = []\n",
        "        max_act_value = -100\n",
        "        for a in movables:\n",
        "            np_action = np.array([[state, a]])\n",
        "            act_value = self.model.predict(np_action)\n",
        "            if act_value > max_act_value:\n",
        "                best_actions = [a,]\n",
        "                max_act_value = act_value\n",
        "            elif act_value == max_act_value:\n",
        "                best_actions.append(a)\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "    def replay_experience(self, batch_size):\n",
        "        batch_size = min(batch_size, len(self.memory))\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        X = []\n",
        "        Y = []\n",
        "        for i in range(batch_size):\n",
        "            state, action, reward, next_state, next_movables, done = minibatch[i]\n",
        "            input_action = [state, action]\n",
        "            if done:\n",
        "                target_f = reward\n",
        "            else:\n",
        "                next_rewards = []\n",
        "                for i in next_movables:\n",
        "                    np_next_s_a = np.array([[next_state, i]])\n",
        "                    next_rewards.append(self.model.predict(np_next_s_a))\n",
        "                np_n_r_max = np.amax(np.array(next_rewards))\n",
        "                target_f = reward + self.gamma * np_n_r_max\n",
        "            X.append(input_action)\n",
        "            Y.append(target_f)\n",
        "        np_X = np.array(X)\n",
        "        np_Y = np.array([Y]).T\n",
        "        self.model.fit(np_X, np_Y, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.e_min:\n",
        "            self.epsilon *= self.e_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VBysXoaSoE6S",
        "colab_type": "code",
        "outputId": "7121f0b1-822f-493c-a023-966080b0469f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "state_size = 2\n",
        "action_size = 2\n",
        "dql_solver = DQN_Solver(state_size, action_size)\n",
        "\n",
        "episodes = 20000\n",
        "times = 1000\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = start_point\n",
        "    score = 0\n",
        "    for time in range(times):\n",
        "        movables = maze_field.get_actions(state)\n",
        "        action = dql_solver.choose_action(state, movables)\n",
        "        reward, done = maze_field.get_val(action)\n",
        "        score = score + reward\n",
        "        next_state = action\n",
        "        next_movables = maze_field.get_actions(next_state)\n",
        "        dql_solver.remember_memory(state, action, reward, next_state, next_movables, done)\n",
        "        if done or time == (times - 1):\n",
        "            if e % 500 == 0:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2} \\t @ {}\"\n",
        "                        .format(e, episodes, score, dql_solver.epsilon, time))\n",
        "            break\n",
        "        state = next_state\n",
        "    dql_solver.replay_experience(32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "episode: 0/20000, score: -374.0, e: 1.0 \t @ 968\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "episode: 500/20000, score: -439.0, e: 0.95 \t @ 999\n",
            "episode: 1000/20000, score: 39.0, e: 0.9 \t @ 24\n",
            "episode: 1500/20000, score: -7.0, e: 0.86 \t @ 158\n",
            "episode: 2000/20000, score: -76.0, e: 0.82 \t @ 258\n",
            "episode: 2500/20000, score: 2.0, e: 0.78 \t @ 134\n",
            "episode: 3000/20000, score: 20.0, e: 0.74 \t @ 72\n",
            "episode: 3500/20000, score: 16.0, e: 0.7 \t @ 72\n",
            "episode: 4000/20000, score: 43.0, e: 0.67 \t @ 28\n",
            "episode: 4500/20000, score: 37.0, e: 0.64 \t @ 34\n",
            "episode: 5000/20000, score: 31.0, e: 0.61 \t @ 32\n",
            "episode: 5500/20000, score: 31.0, e: 0.58 \t @ 48\n",
            "episode: 6000/20000, score: 40.0, e: 0.55 \t @ 24\n",
            "episode: 6500/20000, score: 38.0, e: 0.52 \t @ 40\n",
            "episode: 7000/20000, score: 38.0, e: 0.5 \t @ 22\n",
            "episode: 7500/20000, score: 43.0, e: 0.47 \t @ 16\n",
            "episode: 8000/20000, score: 39.0, e: 0.45 \t @ 26\n",
            "episode: 8500/20000, score: 36.0, e: 0.43 \t @ 28\n",
            "episode: 9000/20000, score: 39.0, e: 0.41 \t @ 36\n",
            "episode: 9500/20000, score: 32.0, e: 0.39 \t @ 38\n",
            "episode: 10000/20000, score: 41.0, e: 0.37 \t @ 22\n",
            "episode: 10500/20000, score: 40.0, e: 0.35 \t @ 28\n",
            "episode: 11000/20000, score: 34.0, e: 0.33 \t @ 36\n",
            "episode: 11500/20000, score: 43.0, e: 0.32 \t @ 20\n",
            "episode: 12000/20000, score: 39.0, e: 0.3 \t @ 20\n",
            "episode: 12500/20000, score: 41.0, e: 0.29 \t @ 18\n",
            "episode: 13000/20000, score: 44.0, e: 0.27 \t @ 18\n",
            "episode: 13500/20000, score: 42.0, e: 0.26 \t @ 24\n",
            "episode: 14000/20000, score: 44.0, e: 0.25 \t @ 18\n",
            "episode: 14500/20000, score: 44.0, e: 0.23 \t @ 14\n",
            "episode: 15000/20000, score: 40.0, e: 0.22 \t @ 22\n",
            "episode: 15500/20000, score: 42.0, e: 0.21 \t @ 16\n",
            "episode: 16000/20000, score: 42.0, e: 0.2 \t @ 16\n",
            "episode: 16500/20000, score: 41.0, e: 0.19 \t @ 20\n",
            "episode: 17000/20000, score: 42.0, e: 0.18 \t @ 16\n",
            "episode: 17500/20000, score: 40.0, e: 0.17 \t @ 24\n",
            "episode: 18000/20000, score: 44.0, e: 0.17 \t @ 14\n",
            "episode: 18500/20000, score: 44.0, e: 0.16 \t @ 14\n",
            "episode: 19000/20000, score: 38.0, e: 0.15 \t @ 20\n",
            "episode: 19500/20000, score: 44.0, e: 0.14 \t @ 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAJcBDw5oE6X",
        "colab_type": "code",
        "outputId": "267c811f-e01c-4172-e82c-c96d1b215838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "state = start_point\n",
        "score = 0\n",
        "steps = 0\n",
        "while True:\n",
        "    steps += 1\n",
        "    movables = maze_field.get_actions(state)\n",
        "    action = dql_solver.choose_best_action(state, movables)\n",
        "    print(\"current state: {0} -> action: {1} \".format(state, action))\n",
        "    reward, done = maze_field.get_val(action)\n",
        "    maze_field.display(state)\n",
        "    score = score + reward\n",
        "    state = action\n",
        "    print(\"current step: {0} \\t score: {1}\\n\".format(steps, score))\n",
        "    if done:\n",
        "        maze_field.display(action)\n",
        "        print(\"goal!\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current state: [0, 1] -> action: [1, 1] \n",
            "\t  #  @@   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 1 \t score: -1.0\n",
            "\n",
            "current state: [1, 1] -> action: [1, 2] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  @@  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 2 \t score: -2.0\n",
            "\n",
            "current state: [1, 2] -> action: [1, 3] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  @@   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 3 \t score: -2.0\n",
            "\n",
            "current state: [1, 3] -> action: [2, 3] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1  @@   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 4 \t score: -2.0\n",
            "\n",
            "current state: [2, 3] -> action: [2, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0  @@  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 5 \t score: -3.0\n",
            "\n",
            "current state: [2, 4] -> action: [3, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  @@  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 6 \t score: -4.0\n",
            "\n",
            "current state: [3, 4] -> action: [4, 4] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  @@   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 7 \t score: -4.0\n",
            "\n",
            "current state: [4, 4] -> action: [4, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1  @@   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 8 \t score: -4.0\n",
            "\n",
            "current state: [4, 5] -> action: [5, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0  @@  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 9 \t score: -4.0\n",
            "\n",
            "current state: [5, 5] -> action: [6, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1  @@   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 10 \t score: -4.0\n",
            "\n",
            "current state: [6, 5] -> action: [7, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1  @@  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 11 \t score: -4.0\n",
            "\n",
            "current state: [7, 5] -> action: [8, 5] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0  @@   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 12 \t score: -5.0\n",
            "\n",
            "current state: [8, 5] -> action: [8, 6] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  @@   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 13 \t score: -5.0\n",
            "\n",
            "current state: [8, 6] -> action: [8, 7] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1  @@  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 14 \t score: -6.0\n",
            "\n",
            "current state: [8, 7] -> action: [9, 7] \n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  @@  -1   # \n",
            "\t  #   #   #   #   #   #   #  50   #   # \n",
            "current step: 15 \t score: 44.0\n",
            "\n",
            "\t  #   S   #   #   #   #   #   #   #   # \n",
            "\t  #  -1  -1   0   #   #   0   0  -1   # \n",
            "\t  #  -1   0   0  -1  -1   #   #   0   # \n",
            "\t  #  -1  -1  -1  -1   0  -1   0  -1   # \n",
            "\t  #   0   #  -1   0   0  -1   0   0   # \n",
            "\t  #  -1   0  -1  -1   0   #  -1   #   # \n",
            "\t  #   0   0   0  -1   0  -1   #   0   # \n",
            "\t  #  -1   0   0   0   0   #  -1   0   # \n",
            "\t  #   0   #   0   0  -1   0  -1  -1   # \n",
            "\t  #   #   #   #   #   #   #  @@   #   # \n",
            "goal!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXf8YuMWoE6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh3sXeFioE6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}